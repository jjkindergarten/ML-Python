{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem1\n",
    "\n",
    "Using Book1 dataset program 3 different waysof crossvalidations (holdout, K = 10 fold cross validation, and leave-one-out cross validation)for 3different types of models: linear regression,decision treeand support vector machinerepressorwith “linear”kernel and your choice of other parameters(do not worry about hyper parametersaccept the model as a “black box”).\n",
    "\n",
    "What can you conclude from cross validations about these 3 models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "data = pd.read_excel('Book1.xlsx', header = None)\n",
    "data = np.transpose(data.as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#small function to make MSE as output of linaer regression\n",
    "def lr_mse(x_train,y_train, x_test, y_test):\n",
    "    def func(x, a, b): return a + b*x\n",
    "    x0 = np.array([0.0,0.0])\n",
    "    optpar = optimization.curve_fit(func,x_train,y_train,x0)\n",
    "    a = optpar[0][0]\n",
    "    b = optpar[0][1]\n",
    "    \n",
    "    #predict\n",
    "    s = 0\n",
    "    if np.size(x_test) == 1:\n",
    "        s = (y_test - (a+b*x_test))**2\n",
    "    else:\n",
    "        for n in range(np.size(x_test)):\n",
    "            s = s + (y_test[n] - (a+b*x_test[n]))**2\n",
    "    return s/np.size(x_test)\n",
    "\n",
    "#small function to make MSE as output of decision tree\n",
    "#depth is the max_depth for decision tree\n",
    "def dt_mse(x_train,y_train, x_test, y_test, depth):\n",
    "    x_train = x_train.reshape(-1,1)\n",
    "    y_train = np.ravel(y_train)\n",
    "    x_test = x_test.reshape(-1,1)\n",
    "    y_test = np.ravel(y_test)\n",
    "    \n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    \n",
    "    tree_reg = DecisionTreeRegressor(max_depth = depth)\n",
    "    tree_reg.fit(x_train,y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_predict = tree_reg.predict(x_test)\n",
    "    s = 0\n",
    "    if np.size(x_test) ==  1:\n",
    "        s = (y_predict - y_test)**2\n",
    "    else:\n",
    "        for n in range(np.size(x_test)):\n",
    "            s = s + (y_predict[n] - y_test[n])**2\n",
    "    return s/np.size(x_test)\n",
    "\n",
    "#small function to make MSE as output of SVM\n",
    "def svm_mse(x_train, y_train, x_test, y_test, c, eps):\n",
    "    x_train = x_train.reshape(-1,1)\n",
    "    y_train = np.ravel(y_train)\n",
    "    x_test = x_test.reshape(-1,1)\n",
    "    y_test = np.ravel(y_test)\n",
    "    \n",
    "    from sklearn.svm import SVR\n",
    "    \n",
    "    svm_linear_reg = SVR(kernel = \"linear\", C = c, epsilon = eps,cache_size = 2000)\n",
    "    svm_linear_reg.fit(x_train, y_train)\n",
    "    \n",
    "    #predict\n",
    "    y_predict = svm_linear_reg.predict(x_test)\n",
    "    s = 0\n",
    "    if np.size(x_test) == 1:\n",
    "        s = (y_predict - y_test)**2\n",
    "    else:\n",
    "        for n in range(np.size(x_test)):\n",
    "            s = s + (y_predict[n] - y_test[n])**2\n",
    "    return s/np.size(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#holdout 20% \n",
    "matr = np.arange(len(data[0]))\n",
    "selection1 = np.random.permutation(matr)\n",
    "num = int(0.8*len(data[0]))\n",
    "selection = [selection1[i] for i in range(0,num)]  \n",
    "selection = np.array(selection)\n",
    "\n",
    "dataTrain_ho = np.array([i for i in np.transpose(data) if i[0] in selection])\n",
    "dataTest_ho = np.array([i for i in np.transpose(data) if i[0] not in selection])\n",
    "dataTrain_ho = np.transpose(dataTrain_ho)\n",
    "dataTest_ho = np.transpose(dataTest_ho)\n",
    "\n",
    "mse_lr_handout = lr_mse(dataTrain_ho[1], dataTrain_ho[2], dataTest_ho[1], dataTest_ho[2])\n",
    "mse_dt_handout = dt_mse(dataTrain_ho[1], dataTrain_ho[2], dataTest_ho[1], dataTest_ho[2], 10)\n",
    "mse_svm_handout = svm_mse(dataTrain_ho[1], dataTrain_ho[2], dataTest_ho[1], dataTest_ho[2],1,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mse generated by the linear regression is: 101.455985291 the mse generated by the decision tree is: 108.095308967 the mse generated by svm is: 101.472671497\n"
     ]
    }
   ],
   "source": [
    "print('the mse generated by the linear regression is:', mse_lr_handout, \n",
    "      'the mse generated by the decision tree is:', mse_dt_handout, 'the mse generated by svm is:', mse_svm_handout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#k fold with k = 10\n",
    "spl = np.array_split(selection1,10)\n",
    "dataTest_fold = list(range(10))\n",
    "dataTrain_fold = list(range(10))\n",
    "for i in range(10):\n",
    "    selection_fold = spl[i]\n",
    "    dataTest_fold_copy = np.array([m for m in np.transpose(data) if m[0] in selection_fold])\n",
    "    dataTrain_fold_copy = np.array([m for m in np.transpose(data) if m[0] not in selection_fold])\n",
    "    dataTrain_fold[i] = np.transpose(dataTrain_fold_copy)\n",
    "    dataTest_fold[i] = np.transpose(dataTest_fold_copy)\n",
    "\n",
    "mse_lr_kfold = 0\n",
    "mse_dt_kfold = 0\n",
    "mse_svm_kfold = 0\n",
    "\n",
    "for i in range(10):\n",
    "    mse_lr_kfold =  mse_lr_kfold +  lr_mse(dataTrain_fold[i][1], dataTrain_fold[i][2], \n",
    "                                           dataTest_fold[i][1], dataTest_fold[i][2])\n",
    "    mse_dt_kfold = mse_dt_kfold + dt_mse(dataTrain_fold[i][1], dataTrain_fold[i][2], \n",
    "                                           dataTest_fold[i][1], dataTest_fold[i][2],10)\n",
    "    mse_svm_kfold =  mse_svm_kfold +  svm_mse(dataTrain_fold[i][1], dataTrain_fold[i][2], \n",
    "                                           dataTest_fold[i][1], dataTest_fold[i][2],1,0.5)\n",
    "mse_lr_kfold = mse_lr_kfold/10\n",
    "mse_dt_kfold = mse_dt_kfold/10\n",
    "mse_svm_kfold = mse_svm_kfold/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mse generated by the linear regression is: 101.605449567 , the mse generated by the decision tree is: 106.687544091 , the mse generated by svm is: 101.607921285\n"
     ]
    }
   ],
   "source": [
    "print(\"the mse generated by the linear regression is:\", mse_lr_kfold,\", the mse generated by the decision tree is:\",\n",
    "     mse_dt_kfold, \", the mse generated by svm is:\", mse_svm_kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#it seems like hard to train svm with a large data set in a short time\n",
    "#for the leave-one-out method, let's turn it into a smaller set\n",
    "\n",
    "#pick up 2000 data randomly from the original dataset, and train with leave-one-out cross vaildation\n",
    "data_for_leave = selection1\n",
    "selection = [selection1[i] for i in range(0,2000)] \n",
    "data_for_leave = np.array([i for i in np.transpose(data) if i[0] in selection])\n",
    "\n",
    "mse_lr_leave_1 = 0\n",
    "mse_dt_leave_1 = 0\n",
    "mse_svm_leave_1 = 0\n",
    "\n",
    "#leave-one-out \n",
    "for i in range(len(data_for_leave)):\n",
    "    dataTest_leave_1_out = data_for_leave[i]\n",
    "    dataTrain_leave_1_out = np.delete(data_for_leave,i,0)\n",
    "    mse_lr_leave_1 = mse_lr_leave_1 + lr_mse(dataTrain_leave_1_out[:,1],dataTrain_leave_1_out[:,2],\n",
    "                                             dataTest_leave_1_out[1],dataTest_leave_1_out[2])\n",
    "    mse_dt_leave_1 = mse_lr_leave_1 + dt_mse(dataTrain_leave_1_out[:,1],dataTrain_leave_1_out[:,2],\n",
    "                                             dataTest_leave_1_out[1],dataTest_leave_1_out[2],10)\n",
    "    mse_svm_leave_1 = mse_svm_leave_1 + svm_mse(dataTrain_leave_1_out[:,1],dataTrain_leave_1_out[:,2],\n",
    "                                             dataTest_leave_1_out[1],dataTest_leave_1_out[2],1,0.5)\n",
    "\n",
    "mse_lr_leave_1 = mse_lr_leave_1/len(data_for_leave)\n",
    "mse_dt_leave_1 = mse_dt_leave_1/len(data_for_leave)\n",
    "mse_svm_leave_1 = mse_svm_leave_1/len(data_for_leave)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mse generated by the linear regression is: 101.950282604 , the mse generated by the decision tree is: 102.05967137828733 , the mse generated by svm is: 101.8790100093584\n"
     ]
    }
   ],
   "source": [
    "print(\"the mse generated by the linear regression is:\", mse_lr_leave_1,\", the mse generated by the decision tree is:\",\n",
    "     float(mse_dt_leave_1), \", the mse generated by svm is:\", float(mse_svm_leave_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As the result shows, the SVM and linear regression method are more suitbale for the data than the decision tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is not much different for the MSE that generated from this three vaildation.\n",
    "I could say that, the result of leave-one-out is most trustable, it won't change with the same dataset. However, it take a great time to complete the cross vaildation. This method is not ideal with huge data or complex model. K-fold is better than simply holdout method, the result of holdout method could change with different pieces of data it picks out, while with k-fold, i can generated a mean of MSE with different pieces of data, and k-fold won't take much time like leave-one-out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
