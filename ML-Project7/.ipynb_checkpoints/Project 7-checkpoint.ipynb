{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 7 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Using dataset (German Credit Data) train to identify good and bad sutomer:\n",
    "\n",
    "    1.voting classifier (using five different predictors and compare weighted vs unweighted voting results)\n",
    "    2.train random forest (random features and training set) selections\n",
    "    3.train adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib.request as ur\n",
    "# url with dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\"\n",
    "# download the file, here i decide to use the original dataset, instead of the one it has quantiflied.\n",
    "raw_data = ur.urlopen(url)\n",
    "text_file = np.loadtxt(raw_data,dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify the catergory col and the numeric col\n",
    "numeric = [1,4,7,10,12,15,17]\n",
    "quantitative = [0,2,3,5,6,8,9,11,13,14,16,18,19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is used to identify whether a string can be converted into flaot\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    " \n",
    "    try:\n",
    "        import unicodedata\n",
    "        unicodedata.numeric(s)\n",
    "        return True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    " \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate the category cols from the numeric cols, and conver the numeric cols type from str to float\n",
    "X_numeric = []\n",
    "X_catergory = []\n",
    "for i in range(len(text_file[0])-1):\n",
    "    if is_number(text_file[0,i]) == True:\n",
    "        col_numeric = [np.float(num) for num in text_file[:,i]]\n",
    "        X_numeric.append(col_numeric)\n",
    "    else:\n",
    "        X_catergory.append(text_file[:,i])\n",
    "y = [np.float(num) for num in text_file[:,20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let me design a function to convert the str into integer. \n",
    "##my idea is to check the string, if meeting the new kind of string, assign a integer to it.\n",
    "##then check what integer, a string matches and then convert the string into that integer \n",
    "def conver_category(data):   #data should be 1-D here\n",
    "    catergory = []\n",
    "    label = {}\n",
    "    new_label = 0\n",
    "    for i in range(len(data)):\n",
    "        if data[i] not in label.keys():\n",
    "            new_label = new_label + 1\n",
    "            label[data[i]] = new_label\n",
    "        catergory.append(label[data[i]])\n",
    "    return catergory, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert catergory into number \n",
    "X_quantitative = []\n",
    "for i in range(len(X_catergory)):\n",
    "    catergory, label = conver_category(X_catergory[i])\n",
    "    X_quantitative.append(catergory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try to convert quantitative col into dummy variable\n",
    "import pandas as pd\n",
    "X_quantitative_dummy = [pd.get_dummies(X_quantitative[i],drop_first = True) for i in range(len(X_quantitative))]\n",
    "X_quantitative_dummy = [X_quantitative_dummy[i].values[:, :] for i in range(len(X_quantitative_dummy))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "i think for the scaling part, i should just scale the numeric col, the quantative part should remain the same. Also, i try two case, one is to treat the category column as real number col, another is to turn the category column into dummy variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note that col 2, 5, 8, 11, 13, 16 and 18 is the numeric columns, let's do the standardlization for it.\n",
    "from sklearn import preprocessing\n",
    "normalized_X_numeric = preprocessing.scale(np.asarray(X_numeric).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine them\n",
    "X_no_dummy = np.hstack((normalized_X_numeric, np.asarray(X_quantitative).T))\n",
    "\n",
    "X_dummy = normalized_X_numeric\n",
    "for i in range(len(X_quantitative_dummy)):\n",
    "    X_dummy = np.hstack((X_dummy, X_quantitative_dummy[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate them to train set and test set\n",
    "num = int(.6*len(X_dummy))\n",
    "#break the dataset\n",
    "X_dummy_train = X_dummy[:num]\n",
    "X_dummy_test = X_dummy[int(num):]\n",
    "\n",
    "X_no_dummy_train = X_no_dummy[:num]\n",
    "X_no_dummy_test = X_no_dummy[int(num):]\n",
    "\n",
    "y_train = np.asarray(y[:num])\n",
    "y_test = np.asarray(y[int(num):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_dummy_test)):\n",
    "    if X_dummy_test.size == 0 :\n",
    "        print ('mistake')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.75      0.92      0.83       277\n",
      "        2.0       0.63      0.32      0.42       123\n",
      "\n",
      "avg / total       0.71      0.73      0.70       400\n",
      "\n",
      "[[254  23]\n",
      " [ 84  39]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.78      0.86      0.82       277\n",
      "        2.0       0.59      0.45      0.51       123\n",
      "\n",
      "avg / total       0.72      0.73      0.72       400\n",
      "\n",
      "[[239  38]\n",
      " [ 68  55]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangj3475/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/wangj3475/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#for voting classifier, i use logistic, naive bayes, knn, svc and decision tree\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "nb_clf = GaussianNB()\n",
    "knn_clf = KNeighborsClassifier()\n",
    "svc_clf = SVC(probability=True)\n",
    "CART_clf = DecisionTreeClassifier()\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('logistic', log_clf))\n",
    "estimators.append(('naive_bayes', nb_clf))\n",
    "estimators.append(('knn', knn_clf))\n",
    "estimators.append(('svc', svc_clf))\n",
    "estimators.append(('CART', CART_clf))\n",
    "\n",
    "#case of using dummy variable \n",
    "#case of using unweighte voting\n",
    "# create the ensemble model\n",
    "ensemble_hard = VotingClassifier(estimators, voting = 'hard')\n",
    "ensemble_hard.fit(X_dummy_train, y_train)\n",
    "ensemble_hard.fit(X_dummy_train, y_train)\n",
    "predicted_hard = ensemble_hard.predict(X_dummy_test)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(y_test, predicted_hard))\n",
    "print(metrics.confusion_matrix(y_test, predicted_hard))\n",
    "\n",
    "#case of using weight voting\n",
    "ensemble_soft = VotingClassifier(estimators, voting = 'soft')\n",
    "ensemble_soft.fit(X_dummy_train, y_train)\n",
    "predicted_soft = ensemble_soft.predict(X_dummy_test)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(y_test, predicted_soft))\n",
    "print(metrics.confusion_matrix(y_test, predicted_soft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.77      0.84      0.80       277\n",
      "        2.0       0.54      0.42      0.47       123\n",
      "\n",
      "avg / total       0.70      0.71      0.70       400\n",
      "\n",
      "[[233  44]\n",
      " [ 71  52]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.77      0.84      0.80       277\n",
      "        2.0       0.54      0.43      0.48       123\n",
      "\n",
      "avg / total       0.70      0.71      0.70       400\n",
      "\n",
      "[[232  45]\n",
      " [ 70  53]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangj3475/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/wangj3475/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#try not dummy variables\n",
    "ensemble_hard.fit(X_no_dummy_train, y_train)\n",
    "predicted_hard = ensemble_hard.predict(X_no_dummy_test)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(y_test, predicted_hard))\n",
    "print(metrics.confusion_matrix(y_test, predicted_hard))\n",
    "\n",
    "ensemble_soft.fit(X_no_dummy_train, y_train)\n",
    "predicted_soft = ensemble_soft.predict(X_no_dummy_test)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(y_test, predicted_soft))\n",
    "print(metrics.confusion_matrix(y_test, predicted_soft))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i think the weighted result is a little bit better than the weighted result. And also, i think using the dummy variable could improve the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Random Forest, i could use the dummy varibale for catergories. Moreover, i could use the package of bagging and randomforest, since there is no random train set options in the random forest, and i would like to compare the result between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.75      0.91      0.82       277\n",
      "        2.0       0.62      0.33      0.43       123\n",
      "\n",
      "avg / total       0.71      0.73      0.70       400\n",
      "\n",
      "[[252  25]\n",
      " [ 82  41]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 500, max_leaf_nodes = 50, max_features=10,\n",
    "                                 oob_score = True, n_jobs = -1)\n",
    "rnd_clf.fit(X_dummy_train,y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_dummy_test)\n",
    "print(metrics.classification_report(y_test, y_pred_rf))\n",
    "print(metrics.confusion_matrix(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.70      0.99      0.82       277\n",
      "        2.0       0.67      0.03      0.06       123\n",
      "\n",
      "avg / total       0.69      0.70      0.59       400\n",
      "\n",
      "[[275   2]\n",
      " [119   4]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(splitter = 'random', max_leaf_nodes = 20), \n",
    "                            n_estimators = 500, max_features = 10, max_samples = 400, bootstrap = True, n_jobs = -1)\n",
    "bag_clf.fit(X_dummy_train,y_train)\n",
    "\n",
    "y_pred_bag = bag_clf.predict(X_dummy_test)\n",
    "print(metrics.classification_report(y_test, y_pred_bag))\n",
    "print(metrics.confusion_matrix(y_test, y_pred_bag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is terrible for bagging result, seldom it would predict the '2' sucessfully. (is this a problem of using random train set instead of all of them ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.80      0.81      0.80       277\n",
      "        2.0       0.55      0.54      0.55       123\n",
      "\n",
      "avg / total       0.72      0.72      0.72       400\n",
      "\n",
      "[[224  53]\n",
      " [ 57  66]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 1), n_estimators = 500, algorithm = 'SAMME.R',\n",
    "                            learning_rate = .5)\n",
    "ada_clf.fit(X_dummy_train,y_train)\n",
    "y_pred_ada = ada_clf.predict(X_dummy_test)\n",
    "print(metrics.classification_report(y_test, y_pred_ada))\n",
    "print(metrics.confusion_matrix(y_test, y_pred_ada))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I thimk the performace of these three algorithms are similar, with precision and recall both around 70%. I prefer using AdaBoost for this dataset because it can predict '2' more accurately than the other two.\n",
    "\n",
    "Also, i think the data size is too small for trainning the random forest and AdaBoost, since the estimators are large, and i use 48 varibales here instead of 19 variables in the dataset, due to dummy varibales. With that much variables, i need more samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
